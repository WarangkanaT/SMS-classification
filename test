import re
import pandas as pd
from pandas import DataFrame

import pythainlp
from pythainlp import word_tokenize
from pythainlp.corpus import thai_stopwords
# from pythainlp.corpus import wordnet
# from pythainlp.spell import correct

import nltk
# from nltk.stem.porter import PorterStemmer
# from nltk.corpus import words
# from stop_words import get_stop_words
# from nltk.corpus import wordnet as wn

# from sklearn.feature_extraction.text import CountVectorizer as cv

import matplotlib as mpl
import matplotlib.font_manager
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
# from sklearn.model_selection import train_test_split

# import sklearn.feature_extraction.text as text
from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm
from sklearn.naive_bayes import BernoulliNB, ComplementNB, MultinomialNB
from sklearn.metrics import accuracy_score, precision_score, confusion_matrix, classification_report

from sklearn.pipeline import Pipeline
import time

#############################################################

def clean(ls_sms):
    temp = []
    for i in ls_sms:
        i = re.sub(r'\s|[*/$฿&/.,;:{}""+-=@#~%^<>()↲!?|_]|\d|[a-z]|[A-Z]','',i)
        temp.append(i)
    ls_sms = temp
    return ls_sms

def token(ls_sms):
    temp = []
    for i in ls_sms:
        words = word_tokenize(i, engine = 'newmm')
        temp.append(words)
    return temp

def remove_stop(temp, added_stop):
    nltk_th_stop = set(pythainlp.corpus.common.thai_stopwords())
    temp3 = []
    for i in temp:
        temp2 = []
        for j in i:
            if j not in nltk_th_stop and not j in added_stop: 
                temp2.append(j)
                x = " ".join(temp2)
        temp3.append(x)
    ls_sms = temp3
    return ls_sms

def bow(ls_sms):
    cv = CountVectorizer(analyzer=lambda x:x.split(' '))
    count_matrix = cv.fit_transform(ls_sms)
    word_matrix = count_matrix.toarray()
    bow = pd.DataFrame(word_matrix, columns = cv.get_feature_names())
    bowsum = bow.sum(axis = 0, skipna = True)
    bow = bowsum.sort_values(ascending = False)[:50]
    return bow

def bow_plot(bow):
    fp = mpl.font_manager.FontProperties(family ='Tahoma', size = 13)
    sns.set_theme(style = 'whitegrid', font = 'tahoma')
    plt.subplots(figsize = (3, 8))
    sns.barplot(bow.values, bow.index)
    plt.ylabel("Words", fontproperties = fp)
    plt.xlabel("Frequencies", fontproperties = fp)
    plt.title("Frequency Distribution")
    return plt.show()

def train_pipe(classifier, train_x, valid_x, train_y, valid_y):
    start = time.time()
    pipe = Pipeline([("vectorizer", TfidfVectorizer()), ("classifier", classifier)])
    pipe.fit(train_x, train_y)
    end = time.time()
    score = pipe.score(valid_x, valid_y)
    return score, start, end

def train(classifier, xtrain_tfidf, train_y, xvalid_tfidf, valid_y):
    start = time.time()
    classifier.fit(xtrain_tfidf, train_y)
    pred_y = classifier.predict(xvalid_tfidf)
    end = time.time()
    accuracy = metrics.accuracy_score(pred_y, valid_y)
    precision = metrics.precision_score(pred_y, valid_y)
    recall = metrics.recall_score(pred_y, valid_y)
    confusion = metrics.confusion_matrix(pred_y, valid_y)
    class_report = metrics.classification_report(pred_y, valid_y)
    return accuracy, precision, recall, confusion, class_report, start, end

#############################################################

# Prepare data
# Import data
data = pd.read_csv("data/sms2.csv", encoding = "utf-8")
# Print sample
print()
print("+"*10 ,"Sample Data", "+"*10)
print(data.head())
print()

# Clean text
data['sms'] = clean(data['sms'])
# Print sample
# print()
# print("+"*10 ,"Sample Cleansing Data", "+"*10)
# print(data.head())
# print()

# Text tokenize
token = token(data['sms'])
# Print sample
# print()
# print("+"*10 ,"Sample Token", "+"*10)
# for i in token[:5]:
#     print(i)
# print()

# Remove stop word
added_stop = ["สวัสดี", "เฉพาะ", "คุณ", "มค", "กพ", "มีค", "เมย", "พค", "มิย", "กค", "สค", "กย", "ตค", "พย", "ธค"]
data['sms'] = remove_stop(token, added_stop)

# Create bag of word
# bow = bow(data['sms'])
# bow_plot(bow)

# Split data into train and validation
train_x, valid_x, train_y, valid_y = model_selection.train_test_split(data["sms"], data["label"], test_size = 0.25, random_state = 0)

# Encode and TFIDF
encoder = preprocessing.LabelEncoder()
train_y = encoder.fit_transform(train_y)
valid_y = encoder.fit_transform(valid_y)
# Uni-gram uncomment here
tfidf_vect = TfidfVectorizer(analyzer = 'word', token_pattern=r'\w{1,}')
# N-gram uncomment here
# tfidf_vect = TfidfVectorizer(ngram_range = (1, 2))
tfidf_vect.fit(data["sms"])
xtrain_tfidf = tfidf_vect.transform(train_x)
xvalid_tfidf = tfidf_vect.transform(valid_x)
xtrain_tfidf.data

#############################################################################

# Training & testing algorithm
# MultinomialNB, MultinomialNB, and ComplementNB with alpha = 1.0, 0.1, 0.01, and 0.001 

# Sample uncomment here
# classifier = [MultinomialNB(alpha = 1.0), BernoulliNB(alpha = 1.0), ComplementNB(alpha = 1.0)]

# Full uncomment here
# classifier = [MultinomialNB(alpha = 1.0), BernoulliNB(alpha = 1.0), ComplementNB(alpha = 1.0), MultinomialNB(alpha = 0.1), BernoulliNB(alpha = 0.1), ComplementNB(alpha = 0.1), MultinomialNB(alpha = 0.01), BernoulliNB(alpha = 0.01), ComplementNB(alpha = 0.01), MultinomialNB(alpha = 0.001), BernoulliNB(alpha = 0.001), ComplementNB(alpha = 0.001)]

# for i in classifier:
#     start = time.time()
#     i.fit(xtrain_tfidf, train_y)
#     pred_y = i.predict(xvalid_tfidf)
#     end = time.time()

#     accuracy = metrics.accuracy_score(pred_y, valid_y)
#     precision = metrics.precision_score(pred_y, valid_y)
#     recall = metrics.recall_score(pred_y, valid_y)
#     confusion = metrics.confusion_matrix(pred_y, valid_y)
#     target_names = ["0 gambling", "1 ok"]
#     class_report = metrics.classification_report(pred_y, valid_y, target_names = target_names)

#     print()
#     print(i)
#     print("Sample row and column: ", xtrain_tfidf.shape)
#     print("Time: ", end - start)
#     print("Accuracy: ", accuracy)
#     print("Precision: ", precision)
#     print("Recall: ", recall)
#     print("Confusion matrix:")
#     print(confusion)
#     print("Classification report:")
#     print(class_report)
#     print("-"*10)

############################################################################

# Selected algorithm

classifier = BernoulliNB(alpha = 0.001)

start = time.time()
classifier.fit(xtrain_tfidf, train_y)
pred_y = classifier.predict(xvalid_tfidf)
end = time.time()

accuracy = metrics.accuracy_score(pred_y, valid_y)
precision = metrics.precision_score(pred_y, valid_y)
recall = metrics.recall_score(pred_y, valid_y)
confusion = metrics.confusion_matrix(pred_y, valid_y)
target_names = ["0 gambling", "1 ok"]
class_report = metrics.classification_report(pred_y, valid_y, target_names = target_names)

############################################################################

# Predict new input

new_sms = []
while (1):
    sms = input("Type your SMS (or enter to cancel/quit): ")
    if sms is not '':
        new_sms.append(sms)
    else:
        break
print()
print("+"*10 ,"Your SMS", "+"*10)
print(new_sms)
print()

new_sms = clean(new_sms)

temp = []
x = []
for i in new_sms:
    words = word_tokenize(i, engine = 'newmm')
    temp.append(words)
temp3 = []
for i in temp:
    temp2 = []
    for j in i:
        temp2.append(j)
        x = " ".join(temp2)
    temp3.append(x)
new_sms = temp3
print(new_sms)

new_sms_cv = tfidf_vect.transform(new_sms)
new_pred_y = classifier.predict(new_sms_cv)

print()
print("+"*10 ,"Prediction", "+"*10)
print(list(encoder.inverse_transform(new_pred_y)))
print()

###########################################################################
